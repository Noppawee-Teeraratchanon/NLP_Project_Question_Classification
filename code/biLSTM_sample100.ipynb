{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.2+cu121', '0.7.1', '0.17.2+cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchdata.__version__, torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234 #change three times\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionType</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>Does this system filter gaillardia?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>Can you get a gas dryer instead of electric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>I have a 2010 maytag centennial washer w101409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>CAN YOU JUST PLUG IT IN ONCE IT IS SEATED?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>does this replace a thermadore water filter ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  questionType                                           question\n",
       "0       yes/no                Does this system filter gaillardia?\n",
       "1       yes/no        Can you get a gas dryer instead of electric\n",
       "2       yes/no  I have a 2010 maytag centennial washer w101409...\n",
       "3       yes/no         CAN YOU JUST PLUG IT IN ONCE IT IS SEATED?\n",
       "4       yes/no  does this replace a thermadore water filter ca..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/pre-category/sample100percategoty.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "df['question']  =  df['question'].apply(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionType</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>does this system filter gaillardia?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>can you get a gas dryer instead of electric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i have a 2010 maytag centennial washer w101409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>can you just plug it in once it is seated?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>does this replace a thermadore water filter ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   questionType                                           question\n",
       "0             0                does this system filter gaillardia?\n",
       "1             0        can you get a gas dryer instead of electric\n",
       "2             0  i have a 2010 maytag centennial washer w101409...\n",
       "3             0         can you just plug it in once it is seated?\n",
       "4             0  does this replace a thermadore water filter ca..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tranform the text question type to integer\n",
    "df['questionType']=df['questionType'].replace(['yes/no','open-ended'], [0,1])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data):\n",
    "    regex_s = re.sub(\"\\\\(.+?\\\\)|[\\r\\n|\\n\\r]|!\", \"\", data)\n",
    "    fin = \" \".join(regex_s.split())\n",
    "    return fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = df['question'].apply(data_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.15,stratify=df['questionType'], random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_df, test_size=0.15, stratify=train_df['questionType'],random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "questionType\n",
       "0    759\n",
       "1    758\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['questionType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "questionType\n",
       "1    158\n",
       "0    157\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['questionType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "questionType\n",
       "0    134\n",
       "1    134\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['questionType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can you use this to get the temperature of wine through the bottle?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['question'][1848]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'is', 'the', 'best', 'product', '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens    = tokenizer(\"What is the best product?\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to integers (numeral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for data_sample in data:\n",
    "        yield tokenizer(data_sample) \n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_df['question']), specials = ['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[842, 11, 7]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'it', 'is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = vocab.get_itos()\n",
    "mapping[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3905"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3905, 300])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWrap(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataframe.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "questionType                                                    1\n",
       "question        do these things cover the front and the back o...\n",
       "Name: 381, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataWrap(train_df)\n",
    "valid = DataWrap(val_df)\n",
    "test = DataWrap(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline  = lambda x: vocab(tokenizer(x)) #{hello world this is yt} => {'hello', 'world', 'this', 'is', 'yt'} => {4, 88, 11, 22, 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 66, 2222, 0, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline(\"I am currently teaching LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate_fn to let each batch has same size\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "pad_idx = vocab['<pad>'] #get index of pad in vocab list\n",
    "# padding for every sentencce in batch to have same length \n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64) # convert to integer before appending\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0))\n",
    "    return torch.tensor(label_list, dtype=torch.int64), pad_sequence(text_list, padding_value=pad_idx, batch_first=True), torch.tensor(length_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  collate_fn=collate_batch) #num_workers to train faster\n",
    "val_loader   = DataLoader(valid, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, text, length in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape #(batch_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 36])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape #(batch_size, seq len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length.shape #(batch_size, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model (biLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, num_layers, \n",
    "                 bidirectional, dropout, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm      = nn.LSTM(\n",
    "                            emb_dim,\n",
    "                            hid_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout = dropout,\n",
    "                            batch_first = True\n",
    "                        )\n",
    "        self.fc        = nn.Linear(hid_dim * 2, output_dim) # time 2 b/c bidirectional, output_dim = 4 since there are 4 class\n",
    "    \n",
    "    def forward(self, text, text_length):\n",
    "        #text = [batch_size, seq len]\n",
    "        embedded = self.embedding(text)\n",
    "        #text = [batch_size, seq len, emb_dim]\n",
    "        \n",
    "        #pack sequence to ignore any padding\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length.to('cpu'), \n",
    "                                                            enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)\n",
    "        #output is basically all the hidden states;  hn is only last hidden state; cn is last cell state\n",
    "        \n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        #output = [batch_size, seq len, hidden_dim * num directions]\n",
    "        #hn     = [num_layers * num_directions, batch_size,  hid_dim]  #3 layers birectional - hn1f, hn1b, hn2f, hn2b, hn3f, hn3b\n",
    "        #cn     = [num_layers * num_directions, batch_size,  hid_dim]\n",
    "        \n",
    "        hn      = torch.cat((hn[-2, :, :], hn[-1, :, :]), dim = 1) # hn3f, hn3b\n",
    "        #hn     = [batch_size, hidden_dim * num_directions]\n",
    "        \n",
    "        return self.fc(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight) # normal distribution\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    predicted  = torch.max(preds.data, 1)[1] #.data for getting value in tensor\n",
    "    batch_corr = (predicted == y).sum()\n",
    "    acc        = batch_corr / len(y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc  = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i, (label, text, text_length) in enumerate(loader):\n",
    "        label = label.to(device)\n",
    "        text  = text.to(device)\n",
    "        \n",
    "        predictions = model(text, text_length).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, label)\n",
    "        acc  = accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc  += acc.item()\n",
    "        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc  = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text, text_length) in enumerate(loader):\n",
    "            label = label.to(device)\n",
    "            text  = text.to(device)\n",
    "            \n",
    "            predictions = model(text, text_length).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc  += acc.item()\n",
    "        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(val_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsfasdg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdsfasdg\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dsfasdg' is not defined"
     ]
    }
   ],
   "source": [
    "dsfasdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/218882049739984480', creation_time=1713171317283, experiment_id='218882049739984480', last_update_time=1713171317283, lifecycle_stage='active', name='question_classification', tags={}>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#experiment tracking\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import os\n",
    "\n",
    "# This the dockerized method.\n",
    "# We build two docker containers, one for python/jupyter and another for mlflow.\n",
    "# The url `mlflow` is resolved into another container within the same composer.\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "# In the dockerized way, the user who runs this code will be `root`.\n",
    "# The MLflow will also log the run user_id as `root`.\n",
    "# To change that, we need to set this environ[\"LOGNAME\"] to your name.\n",
    "os.environ[\"LOGNAME\"] = \"noppawee\"\n",
    "#mlflow.create_experiment(name=\"noppawee-ML-project\")  #create if you haven't create\n",
    "mlflow.set_experiment(experiment_name=\"question_classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== biLSTM with 5-epochs-128-hidden dim-2-num layers =====\n",
      "Epoch: 1 | Time: 0m 2s\n",
      "\tTrain Loss: 0.575 | Train Acc: 71.11%\n",
      "\tVal.  Loss: 0.348 | Val Acc: 82.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Time: 0m 2s\n",
      "\tTrain Loss: 0.350 | Train Acc: 84.73%\n",
      "\tVal.  Loss: 0.315 | Val Acc: 85.31%\n",
      "Epoch: 3 | Time: 0m 1s\n",
      "\tTrain Loss: 0.177 | Train Acc: 93.11%\n",
      "\tVal.  Loss: 0.304 | Val Acc: 84.48%\n",
      "Epoch: 4 | Time: 0m 1s\n",
      "\tTrain Loss: 0.121 | Train Acc: 95.55%\n",
      "\tVal.  Loss: 0.294 | Val Acc: 87.03%\n",
      "Epoch: 5 | Time: 0m 2s\n",
      "\tTrain Loss: 0.063 | Train Acc: 97.85%\n",
      "\tVal.  Loss: 0.334 | Val Acc: 86.27%\n",
      "===== biLSTM with 5-epochs-128-hidden dim-4-num layers =====\n",
      "Epoch: 1 | Time: 0m 3s\n",
      "\tTrain Loss: 0.519 | Train Acc: 73.04%\n",
      "\tVal.  Loss: 0.710 | Val Acc: 78.07%\n",
      "Epoch: 2 | Time: 0m 4s\n",
      "\tTrain Loss: 0.192 | Train Acc: 92.51%\n",
      "\tVal.  Loss: 0.460 | Val Acc: 81.82%\n",
      "Epoch: 3 | Time: 0m 4s\n",
      "\tTrain Loss: 0.063 | Train Acc: 97.95%\n",
      "\tVal.  Loss: 0.515 | Val Acc: 81.56%\n",
      "Epoch: 4 | Time: 0m 5s\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.96%\n",
      "\tVal.  Loss: 0.648 | Val Acc: 82.78%\n",
      "Epoch: 5 | Time: 0m 4s\n",
      "\tTrain Loss: 0.010 | Train Acc: 99.45%\n",
      "\tVal.  Loss: 0.850 | Val Acc: 82.18%\n",
      "===== biLSTM with 5-epochs-128-hidden dim-6-num layers =====\n",
      "Epoch: 1 | Time: 0m 5s\n",
      "\tTrain Loss: 0.479 | Train Acc: 72.54%\n",
      "\tVal.  Loss: 0.676 | Val Acc: 78.33%\n",
      "Epoch: 2 | Time: 0m 5s\n",
      "\tTrain Loss: 0.075 | Train Acc: 97.11%\n",
      "\tVal.  Loss: 0.811 | Val Acc: 79.34%\n",
      "Epoch: 3 | Time: 0m 5s\n",
      "\tTrain Loss: 0.027 | Train Acc: 98.89%\n",
      "\tVal.  Loss: 0.821 | Val Acc: 80.57%\n",
      "Epoch: 4 | Time: 0m 5s\n",
      "\tTrain Loss: 0.019 | Train Acc: 99.26%\n",
      "\tVal.  Loss: 0.900 | Val Acc: 80.59%\n",
      "Epoch: 5 | Time: 0m 5s\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.54%\n",
      "\tVal.  Loss: 1.432 | Val Acc: 78.64%\n",
      "===== biLSTM with 5-epochs-256-hidden dim-2-num layers =====\n",
      "Epoch: 1 | Time: 0m 3s\n",
      "\tTrain Loss: 0.433 | Train Acc: 88.55%\n",
      "\tVal.  Loss: 0.412 | Val Acc: 80.91%\n",
      "Epoch: 2 | Time: 0m 3s\n",
      "\tTrain Loss: 0.097 | Train Acc: 97.20%\n",
      "\tVal.  Loss: 0.598 | Val Acc: 75.23%\n",
      "Epoch: 3 | Time: 0m 3s\n",
      "\tTrain Loss: 0.025 | Train Acc: 99.48%\n",
      "\tVal.  Loss: 0.905 | Val Acc: 75.85%\n",
      "Epoch: 4 | Time: 0m 3s\n",
      "\tTrain Loss: 0.005 | Train Acc: 99.93%\n",
      "\tVal.  Loss: 0.867 | Val Acc: 77.05%\n",
      "Epoch: 5 | Time: 0m 3s\n",
      "\tTrain Loss: 0.002 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.050 | Val Acc: 76.14%\n",
      "===== biLSTM with 5-epochs-256-hidden dim-4-num layers =====\n",
      "Epoch: 1 | Time: 0m 6s\n",
      "\tTrain Loss: 0.331 | Train Acc: 87.61%\n",
      "\tVal.  Loss: 0.640 | Val Acc: 76.19%\n",
      "Epoch: 2 | Time: 0m 7s\n",
      "\tTrain Loss: 0.052 | Train Acc: 98.31%\n",
      "\tVal.  Loss: 1.011 | Val Acc: 81.19%\n",
      "Epoch: 3 | Time: 0m 6s\n",
      "\tTrain Loss: 0.004 | Train Acc: 99.93%\n",
      "\tVal.  Loss: 1.550 | Val Acc: 76.87%\n",
      "Epoch: 4 | Time: 0m 6s\n",
      "\tTrain Loss: 0.001 | Train Acc: 99.93%\n",
      "\tVal.  Loss: 1.547 | Val Acc: 80.57%\n",
      "Epoch: 5 | Time: 0m 7s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.71%\n",
      "\tVal.  Loss: 0.847 | Val Acc: 77.13%\n",
      "===== biLSTM with 5-epochs-256-hidden dim-6-num layers =====\n",
      "Epoch: 1 | Time: 0m 10s\n",
      "\tTrain Loss: 0.706 | Train Acc: 88.18%\n",
      "\tVal.  Loss: 6.974 | Val Acc: 62.96%\n",
      "Epoch: 2 | Time: 0m 10s\n",
      "\tTrain Loss: 0.579 | Train Acc: 79.63%\n",
      "\tVal.  Loss: 0.646 | Val Acc: 60.35%\n",
      "Epoch: 3 | Time: 0m 11s\n",
      "\tTrain Loss: 0.635 | Train Acc: 57.76%\n",
      "\tVal.  Loss: 0.606 | Val Acc: 78.77%\n",
      "Epoch: 4 | Time: 0m 10s\n",
      "\tTrain Loss: 0.415 | Train Acc: 83.73%\n",
      "\tVal.  Loss: 0.410 | Val Acc: 80.93%\n",
      "Epoch: 5 | Time: 0m 12s\n",
      "\tTrain Loss: 0.262 | Train Acc: 89.24%\n",
      "\tVal.  Loss: 0.399 | Val Acc: 80.93%\n",
      "===== biLSTM with 5-epochs-512-hidden dim-2-num layers =====\n",
      "Epoch: 1 | Time: 0m 10s\n",
      "\tTrain Loss: 0.199 | Train Acc: 92.12%\n",
      "\tVal.  Loss: 0.696 | Val Acc: 78.41%\n",
      "Epoch: 2 | Time: 0m 8s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.61%\n",
      "\tVal.  Loss: 1.159 | Val Acc: 76.53%\n",
      "Epoch: 3 | Time: 0m 9s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.764 | Val Acc: 75.23%\n",
      "Epoch: 4 | Time: 0m 9s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 2.028 | Val Acc: 75.57%\n",
      "Epoch: 5 | Time: 0m 8s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 2.150 | Val Acc: 74.58%\n",
      "===== biLSTM with 5-epochs-512-hidden dim-4-num layers =====\n",
      "Epoch: 1 | Time: 0m 19s\n",
      "\tTrain Loss: 0.260 | Train Acc: 86.85%\n",
      "\tVal.  Loss: 1.426 | Val Acc: 77.49%\n",
      "Epoch: 2 | Time: 0m 18s\n",
      "\tTrain Loss: 0.070 | Train Acc: 97.98%\n",
      "\tVal.  Loss: 0.800 | Val Acc: 78.20%\n",
      "Epoch: 3 | Time: 0m 18s\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.67%\n",
      "\tVal.  Loss: 0.887 | Val Acc: 77.55%\n",
      "Epoch: 4 | Time: 0m 18s\n",
      "\tTrain Loss: 0.002 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.441 | Val Acc: 78.15%\n",
      "Epoch: 5 | Time: 0m 19s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.667 | Val Acc: 78.15%\n",
      "===== biLSTM with 5-epochs-512-hidden dim-6-num layers =====\n",
      "Epoch: 1 | Time: 0m 32s\n",
      "\tTrain Loss: 0.229 | Train Acc: 91.73%\n",
      "\tVal.  Loss: 1.259 | Val Acc: 76.50%\n",
      "Epoch: 2 | Time: 0m 34s\n",
      "\tTrain Loss: 0.027 | Train Acc: 99.15%\n",
      "\tVal.  Loss: 1.530 | Val Acc: 77.39%\n",
      "Epoch: 3 | Time: 0m 31s\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.35%\n",
      "\tVal.  Loss: 1.171 | Val Acc: 79.63%\n",
      "Epoch: 4 | Time: 0m 31s\n",
      "\tTrain Loss: 0.035 | Train Acc: 99.19%\n",
      "\tVal.  Loss: 1.361 | Val Acc: 75.85%\n",
      "Epoch: 5 | Time: 0m 29s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.93%\n",
      "\tVal.  Loss: 1.448 | Val Acc: 75.83%\n",
      "===== biLSTM with 10-epochs-128-hidden dim-2-num layers =====\n",
      "Epoch: 1 | Time: 0m 1s\n",
      "\tTrain Loss: 0.243 | Train Acc: 92.19%\n",
      "\tVal.  Loss: 3.675 | Val Acc: 72.67%\n",
      "Epoch: 2 | Time: 0m 1s\n",
      "\tTrain Loss: 0.129 | Train Acc: 98.11%\n",
      "\tVal.  Loss: 2.805 | Val Acc: 66.79%\n",
      "Epoch: 3 | Time: 0m 2s\n",
      "\tTrain Loss: 0.066 | Train Acc: 98.63%\n",
      "\tVal.  Loss: 1.027 | Val Acc: 72.13%\n",
      "Epoch: 4 | Time: 0m 1s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.48%\n",
      "\tVal.  Loss: 0.935 | Val Acc: 73.74%\n",
      "Epoch: 5 | Time: 0m 1s\n",
      "\tTrain Loss: 0.003 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.058 | Val Acc: 72.78%\n",
      "Epoch: 6 | Time: 0m 1s\n",
      "\tTrain Loss: 0.002 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.176 | Val Acc: 74.66%\n",
      "Epoch: 7 | Time: 0m 1s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.248 | Val Acc: 74.34%\n",
      "Epoch: 8 | Time: 0m 1s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.309 | Val Acc: 73.72%\n",
      "Epoch: 9 | Time: 0m 1s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.358 | Val Acc: 73.38%\n",
      "Epoch: 10 | Time: 0m 2s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.401 | Val Acc: 73.38%\n",
      "===== biLSTM with 10-epochs-128-hidden dim-4-num layers =====\n",
      "Epoch: 1 | Time: 0m 4s\n",
      "\tTrain Loss: 0.241 | Train Acc: 93.03%\n",
      "\tVal.  Loss: 3.156 | Val Acc: 74.58%\n",
      "Epoch: 2 | Time: 0m 3s\n",
      "\tTrain Loss: 0.081 | Train Acc: 98.37%\n",
      "\tVal.  Loss: 1.438 | Val Acc: 76.19%\n",
      "Epoch: 3 | Time: 0m 3s\n",
      "\tTrain Loss: 0.003 | Train Acc: 99.93%\n",
      "\tVal.  Loss: 1.730 | Val Acc: 71.17%\n",
      "Epoch: 4 | Time: 0m 4s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.765 | Val Acc: 72.42%\n",
      "Epoch: 5 | Time: 0m 3s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.845 | Val Acc: 72.75%\n",
      "Epoch: 6 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.905 | Val Acc: 72.44%\n",
      "Epoch: 7 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.962 | Val Acc: 72.13%\n",
      "Epoch: 8 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 2.005 | Val Acc: 72.13%\n",
      "Epoch: 9 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 2.044 | Val Acc: 72.13%\n",
      "Epoch: 10 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 2.086 | Val Acc: 72.13%\n",
      "===== biLSTM with 10-epochs-128-hidden dim-6-num layers =====\n",
      "Epoch: 1 | Time: 0m 5s\n",
      "\tTrain Loss: 0.290 | Train Acc: 86.59%\n",
      "\tVal.  Loss: 3.410 | Val Acc: 71.09%\n",
      "Epoch: 2 | Time: 0m 5s\n",
      "\tTrain Loss: 0.240 | Train Acc: 95.56%\n",
      "\tVal.  Loss: 0.696 | Val Acc: 76.58%\n",
      "Epoch: 3 | Time: 0m 5s\n",
      "\tTrain Loss: 0.074 | Train Acc: 98.54%\n",
      "\tVal.  Loss: 1.153 | Val Acc: 77.78%\n",
      "Epoch: 4 | Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 99.54%\n",
      "\tVal.  Loss: 1.387 | Val Acc: 74.60%\n",
      "Epoch: 5 | Time: 0m 5s\n",
      "\tTrain Loss: 0.165 | Train Acc: 96.01%\n",
      "\tVal.  Loss: 0.880 | Val Acc: 68.33%\n",
      "Epoch: 6 | Time: 0m 5s\n",
      "\tTrain Loss: 0.151 | Train Acc: 96.96%\n",
      "\tVal.  Loss: 0.826 | Val Acc: 76.19%\n",
      "Epoch: 7 | Time: 0m 5s\n",
      "\tTrain Loss: 0.129 | Train Acc: 96.85%\n",
      "\tVal.  Loss: 0.880 | Val Acc: 76.82%\n",
      "Epoch: 8 | Time: 0m 5s\n",
      "\tTrain Loss: 0.112 | Train Acc: 97.21%\n",
      "\tVal.  Loss: 1.013 | Val Acc: 80.28%\n",
      "Epoch: 9 | Time: 0m 4s\n",
      "\tTrain Loss: 0.098 | Train Acc: 97.59%\n",
      "\tVal.  Loss: 0.981 | Val Acc: 75.57%\n",
      "Epoch: 10 | Time: 0m 5s\n",
      "\tTrain Loss: 0.096 | Train Acc: 97.63%\n",
      "\tVal.  Loss: 1.102 | Val Acc: 77.10%\n",
      "===== biLSTM with 10-epochs-256-hidden dim-2-num layers =====\n",
      "Epoch: 1 | Time: 0m 3s\n",
      "\tTrain Loss: 0.363 | Train Acc: 92.45%\n",
      "\tVal.  Loss: 3.216 | Val Acc: 73.64%\n",
      "Epoch: 2 | Time: 0m 3s\n",
      "\tTrain Loss: 0.046 | Train Acc: 99.00%\n",
      "\tVal.  Loss: 1.786 | Val Acc: 72.39%\n",
      "Epoch: 3 | Time: 0m 3s\n",
      "\tTrain Loss: 0.009 | Train Acc: 99.74%\n",
      "\tVal.  Loss: 1.377 | Val Acc: 77.44%\n",
      "Epoch: 4 | Time: 0m 3s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.356 | Val Acc: 77.10%\n",
      "Epoch: 5 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.400 | Val Acc: 76.76%\n",
      "Epoch: 6 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.443 | Val Acc: 76.76%\n",
      "Epoch: 7 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.477 | Val Acc: 76.76%\n",
      "Epoch: 8 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.507 | Val Acc: 76.76%\n",
      "Epoch: 9 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.532 | Val Acc: 76.76%\n",
      "Epoch: 10 | Time: 0m 3s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.555 | Val Acc: 76.76%\n",
      "===== biLSTM with 10-epochs-256-hidden dim-4-num layers =====\n",
      "Epoch: 1 | Time: 0m 6s\n",
      "\tTrain Loss: 0.252 | Train Acc: 90.86%\n",
      "\tVal.  Loss: 4.339 | Val Acc: 71.42%\n",
      "Epoch: 2 | Time: 0m 6s\n",
      "\tTrain Loss: 0.175 | Train Acc: 95.97%\n",
      "\tVal.  Loss: 0.726 | Val Acc: 63.37%\n",
      "Epoch: 3 | Time: 0m 6s\n",
      "\tTrain Loss: 0.298 | Train Acc: 90.64%\n",
      "\tVal.  Loss: 0.639 | Val Acc: 50.59%\n",
      "Epoch: 4 | Time: 0m 7s\n",
      "\tTrain Loss: 0.248 | Train Acc: 91.90%\n",
      "\tVal.  Loss: 0.611 | Val Acc: 80.62%\n",
      "Epoch: 5 | Time: 0m 7s\n",
      "\tTrain Loss: 0.070 | Train Acc: 98.37%\n",
      "\tVal.  Loss: 1.124 | Val Acc: 74.29%\n",
      "Epoch: 6 | Time: 0m 7s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.74%\n",
      "\tVal.  Loss: 1.006 | Val Acc: 79.37%\n",
      "Epoch: 7 | Time: 0m 8s\n",
      "\tTrain Loss: 0.027 | Train Acc: 99.45%\n",
      "\tVal.  Loss: 1.862 | Val Acc: 64.73%\n",
      "Epoch: 8 | Time: 0m 8s\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.02%\n",
      "\tVal.  Loss: 1.629 | Val Acc: 69.52%\n",
      "Epoch: 9 | Time: 0m 7s\n",
      "\tTrain Loss: 0.009 | Train Acc: 99.84%\n",
      "\tVal.  Loss: 1.188 | Val Acc: 71.45%\n",
      "Epoch: 10 | Time: 0m 6s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.74%\n",
      "\tVal.  Loss: 1.253 | Val Acc: 75.23%\n",
      "===== biLSTM with 10-epochs-256-hidden dim-6-num layers =====\n",
      "Epoch: 1 | Time: 0m 11s\n",
      "\tTrain Loss: 0.143 | Train Acc: 91.86%\n",
      "\tVal.  Loss: 4.234 | Val Acc: 79.37%\n",
      "Epoch: 2 | Time: 0m 10s\n",
      "\tTrain Loss: 0.589 | Train Acc: 75.59%\n",
      "\tVal.  Loss: 0.522 | Val Acc: 71.17%\n",
      "Epoch: 3 | Time: 0m 9s\n",
      "\tTrain Loss: 0.513 | Train Acc: 73.00%\n",
      "\tVal.  Loss: 0.578 | Val Acc: 65.35%\n",
      "Epoch: 4 | Time: 0m 12s\n",
      "\tTrain Loss: 0.640 | Train Acc: 57.15%\n",
      "\tVal.  Loss: 0.699 | Val Acc: 49.88%\n",
      "Epoch: 5 | Time: 0m 12s\n",
      "\tTrain Loss: 0.690 | Train Acc: 52.46%\n",
      "\tVal.  Loss: 0.699 | Val Acc: 50.04%\n",
      "Epoch: 6 | Time: 0m 12s\n",
      "\tTrain Loss: 0.690 | Train Acc: 51.28%\n",
      "\tVal.  Loss: 0.698 | Val Acc: 49.13%\n",
      "Epoch: 7 | Time: 0m 12s\n",
      "\tTrain Loss: 0.689 | Train Acc: 51.93%\n",
      "\tVal.  Loss: 0.698 | Val Acc: 49.88%\n",
      "Epoch: 8 | Time: 0m 12s\n",
      "\tTrain Loss: 0.688 | Train Acc: 53.04%\n",
      "\tVal.  Loss: 0.696 | Val Acc: 64.76%\n",
      "Epoch: 9 | Time: 0m 12s\n",
      "\tTrain Loss: 0.689 | Train Acc: 52.88%\n",
      "\tVal.  Loss: 0.700 | Val Acc: 49.88%\n",
      "Epoch: 10 | Time: 0m 12s\n",
      "\tTrain Loss: 0.688 | Train Acc: 56.00%\n",
      "\tVal.  Loss: 0.695 | Val Acc: 62.49%\n",
      "===== biLSTM with 10-epochs-512-hidden dim-2-num layers =====\n",
      "Epoch: 1 | Time: 0m 11s\n",
      "\tTrain Loss: 0.267 | Train Acc: 93.75%\n",
      "\tVal.  Loss: 0.834 | Val Acc: 78.82%\n",
      "Epoch: 2 | Time: 0m 10s\n",
      "\tTrain Loss: 0.008 | Train Acc: 99.93%\n",
      "\tVal.  Loss: 1.172 | Val Acc: 76.53%\n",
      "Epoch: 3 | Time: 0m 11s\n",
      "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.484 | Val Acc: 78.77%\n",
      "Epoch: 4 | Time: 0m 11s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.563 | Val Acc: 79.11%\n",
      "Epoch: 5 | Time: 0m 11s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.606 | Val Acc: 79.11%\n",
      "Epoch: 6 | Time: 0m 11s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.648 | Val Acc: 79.11%\n",
      "Epoch: 7 | Time: 0m 10s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.683 | Val Acc: 79.11%\n",
      "Epoch: 8 | Time: 0m 10s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.716 | Val Acc: 78.80%\n",
      "Epoch: 9 | Time: 0m 10s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.745 | Val Acc: 78.80%\n",
      "Epoch: 10 | Time: 0m 10s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.776 | Val Acc: 78.49%\n",
      "===== biLSTM with 10-epochs-512-hidden dim-4-num layers =====\n",
      "Epoch: 1 | Time: 0m 23s\n",
      "\tTrain Loss: 0.382 | Train Acc: 93.72%\n",
      "\tVal.  Loss: 1.398 | Val Acc: 69.26%\n",
      "Epoch: 2 | Time: 0m 23s\n",
      "\tTrain Loss: 0.125 | Train Acc: 96.13%\n",
      "\tVal.  Loss: 0.722 | Val Acc: 80.96%\n",
      "Epoch: 3 | Time: 0m 22s\n",
      "\tTrain Loss: 0.061 | Train Acc: 98.31%\n",
      "\tVal.  Loss: 1.143 | Val Acc: 78.80%\n",
      "Epoch: 4 | Time: 0m 24s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.67%\n",
      "\tVal.  Loss: 1.477 | Val Acc: 77.18%\n",
      "Epoch: 5 | Time: 0m 21s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.80%\n",
      "\tVal.  Loss: 1.656 | Val Acc: 71.76%\n",
      "Epoch: 6 | Time: 0m 22s\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.35%\n",
      "\tVal.  Loss: 1.344 | Val Acc: 75.59%\n",
      "Epoch: 7 | Time: 0m 25s\n",
      "\tTrain Loss: 0.007 | Train Acc: 99.87%\n",
      "\tVal.  Loss: 1.874 | Val Acc: 75.62%\n",
      "Epoch: 8 | Time: 0m 22s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 2.117 | Val Acc: 75.31%\n",
      "Epoch: 9 | Time: 0m 20s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 2.123 | Val Acc: 74.94%\n",
      "Epoch: 10 | Time: 0m 19s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 2.116 | Val Acc: 75.25%\n",
      "===== biLSTM with 10-epochs-512-hidden dim-6-num layers =====\n",
      "Epoch: 1 | Time: 0m 31s\n",
      "\tTrain Loss: 0.358 | Train Acc: 89.20%\n",
      "\tVal.  Loss: 0.920 | Val Acc: 79.97%\n",
      "Epoch: 2 | Time: 0m 30s\n",
      "\tTrain Loss: 0.119 | Train Acc: 97.17%\n",
      "\tVal.  Loss: 1.955 | Val Acc: 78.12%\n",
      "Epoch: 3 | Time: 0m 30s\n",
      "\tTrain Loss: 0.079 | Train Acc: 98.67%\n",
      "\tVal.  Loss: 0.682 | Val Acc: 77.75%\n",
      "Epoch: 4 | Time: 0m 31s\n",
      "\tTrain Loss: 0.126 | Train Acc: 96.75%\n",
      "\tVal.  Loss: 0.778 | Val Acc: 78.07%\n",
      "Epoch: 5 | Time: 0m 30s\n",
      "\tTrain Loss: 0.061 | Train Acc: 98.48%\n",
      "\tVal.  Loss: 1.090 | Val Acc: 79.40%\n",
      "Epoch: 6 | Time: 0m 32s\n",
      "\tTrain Loss: 0.002 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.891 | Val Acc: 75.59%\n",
      "Epoch: 7 | Time: 0m 35s\n",
      "\tTrain Loss: 0.009 | Train Acc: 99.87%\n",
      "\tVal.  Loss: 1.452 | Val Acc: 77.49%\n",
      "Epoch: 8 | Time: 0m 34s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.744 | Val Acc: 75.25%\n",
      "Epoch: 9 | Time: 0m 36s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.860 | Val Acc: 75.25%\n",
      "Epoch: 10 | Time: 0m 33s\n",
      "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
      "\tVal.  Loss: 1.931 | Val Acc: 75.25%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = [5,10]\n",
    "hid_dims = [128,256,512]\n",
    "num_layers = [2,4,6]\n",
    "\n",
    "for num_epoch in num_epochs:\n",
    "    for hid_dim in hid_dims:\n",
    "        for num_layer in num_layers:\n",
    "            input_dim = len(vocab)\n",
    "            emb_dim   = 300 #fasttext\n",
    "            output_dim = 2 #2 types of question\n",
    "            bidirectional = True\n",
    "            dropout    = 0.5  # dropout between layers\n",
    "\n",
    "            params={\"model\":\"biLSTM\", \"num_epochs\":num_epoch, \"input_dim\":input_dim, \"hid_dim\":hid_dim, \"emb_dim\":emb_dim, \"output_dim\":output_dim, \"num_layers\":num_layer, \"dropout\":0.5}\n",
    "            mlflow.start_run(run_name=f\"biLSTM100-{params['num_epochs']}-epochs-{params['hid_dim']}-hidden dim-{params['num_layers']}-num layers\")\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            print(\"=\"*5, f\"biLSTM with {params['num_epochs']}-epochs-{params['hid_dim']}-hidden dim-{params['num_layers']}-num layers\",\"=\"*5)\n",
    "\n",
    "            model = LSTM(input_dim, emb_dim, hid_dim, num_layer, bidirectional, dropout, output_dim)\n",
    "            model.apply(initialize_weight)\n",
    "            model.embedding.weight.data = fast_embedding\n",
    "\n",
    "            lr = 1e-3\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            train_losses, train_accs, val_losses, val_accs = [],[],[],[]\n",
    "            best_valid_loss = float('inf')\n",
    "\n",
    "            for epoch in range(num_epoch):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
    "                valid_loss, valid_acc = evaluate(model, val_loader, criterion, val_loader_length)\n",
    "                \n",
    "                #for plotting\n",
    "                train_losses.append(train_loss)\n",
    "                train_accs.append(train_acc)\n",
    "                val_losses.append(valid_loss)\n",
    "                val_accs.append(valid_acc)\n",
    "                \n",
    "                end_time = time.time()\n",
    "                \n",
    "                epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "                mlflow.log_metric(key=\"train_loss\", value=train_loss, step=epoch)\n",
    "                mlflow.log_metric(key=\"train_acc\", value=train_acc, step=epoch)\n",
    "                mlflow.log_metric(key=\"val_loss\", value=valid_loss, step=epoch)\n",
    "                mlflow.log_metric(key=\"val_acc\", value=valid_acc, step=epoch)\n",
    "                \n",
    "            \n",
    "                #early stopping\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    mlflow.pytorch.log_model(model, \"model\")\n",
    "                \n",
    "                print(f'Epoch: {epoch+1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "                print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "                print(f'\\tVal.  Loss: {valid_loss:.3f} | Val Acc: {valid_acc*100:.2f}%')\n",
    "            mlflow.log_metric(key=\"min_val_loss\", value=min(val_losses), step=epoch)    \n",
    "            mlflow.end_run()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
